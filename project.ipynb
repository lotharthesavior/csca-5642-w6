{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting NBA Players' Career Average Points Per Game\n",
    "## Problem Description\n",
    "This project aims to predict NBA players' career average points per game (PTS) using a deep learning regression model. We leverage player attributes such as height, weight, position, draft details, and college to build a feed-forward neural network (FNN), compare it with a convolutional neural network (CNN) and XGBoost, and evaluate their performance for applications in talent scouting. The dataset is sourced from the [NBA Players Dataset](https://www.kaggle.com/datasets/saunakghosh/nba-players-dataset), aggregated to compute career averages.\n",
    "\n",
    "## Objectives\n",
    "- Perform exploratory data analysis (EDA) to understand data distributions and correlations.\n",
    "- Build and compare three models: FNN, CNN, and XGBoost.\n",
    "- Evaluate models using RMSE and RÂ² metrics.\n",
    "- Discuss implications for NBA talent scouting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "We use the NBA Players Dataset from Kaggle, which includes player attributes and seasonal statistics. The data is aggregated by player to compute career average PTS. Missing values are handled during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.12.0 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (2.12.0)\n",
      "Requirement already satisfied: numpy==1.23.5 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (1.23.5)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (25.2.10)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (1.67.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (3.13.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (0.4.30)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (2.12.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (4.25.7)\n",
      "Requirement already satisfied: setuptools in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (1.17.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (2.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (4.13.2)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorflow==2.12.0) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
      "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.30)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.5.1)\n",
      "Requirement already satisfied: scipy>=1.9 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.15.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.39.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (4.13.2)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.1-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from xgboost) (2.26.2)\n",
      "Requirement already satisfied: scipy in /home/savior/anaconda3/envs/dev/lib/python3.11/site-packages (from xgboost) (1.15.2)\n",
      "Downloading xgboost-3.0.1-py3-none-manylinux_2_28_x86_64.whl (253.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m253.9/253.9 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.12.0 numpy==1.23.5\n",
    "!pip install \"typing-extensions>=4.8.0\"\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERSON_ID</th>\n",
       "      <th>PTS</th>\n",
       "      <th>HEIGHT</th>\n",
       "      <th>WEIGHT</th>\n",
       "      <th>POSITION</th>\n",
       "      <th>COLLEGE</th>\n",
       "      <th>DRAFT_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>14.1</td>\n",
       "      <td>6-4</td>\n",
       "      <td>205.0</td>\n",
       "      <td>G</td>\n",
       "      <td>Arizona State</td>\n",
       "      <td>1983.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6-9</td>\n",
       "      <td>250.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Eastern Michigan</td>\n",
       "      <td>1988.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>7.7</td>\n",
       "      <td>6-11</td>\n",
       "      <td>260.0</td>\n",
       "      <td>C</td>\n",
       "      <td>Syracuse</td>\n",
       "      <td>1981.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6-2</td>\n",
       "      <td>185.0</td>\n",
       "      <td>G</td>\n",
       "      <td>West Virginia Tech</td>\n",
       "      <td>1983.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>6.7</td>\n",
       "      <td>6-8</td>\n",
       "      <td>215.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Wake Forest</td>\n",
       "      <td>1992.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PERSON_ID   PTS HEIGHT  WEIGHT POSITION             COLLEGE  DRAFT_YEAR\n",
       "0          2  14.1    6-4   205.0        G       Arizona State      1983.0\n",
       "1          3   9.5    6-9   250.0        F    Eastern Michigan      1988.0\n",
       "2          7   7.7   6-11   260.0        C            Syracuse      1981.0\n",
       "3          9   9.8    6-2   185.0        G  West Virginia Tech      1983.0\n",
       "4         12   6.7    6-8   215.0        F         Wake Forest      1992.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('./data/PlayerIndex_nba_stats.csv')\n",
    "\n",
    "# Aggregate seasonal data to compute career average PTS\n",
    "career_data = data.groupby('PERSON_ID').agg({\n",
    "    'PTS': 'mean',\n",
    "    'HEIGHT': 'first',\n",
    "    'WEIGHT': 'first',\n",
    "    'POSITION': 'first',\n",
    "    'COLLEGE': 'first',\n",
    "    'DRAFT_YEAR': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Display first few rows\n",
    "career_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "We inspect the dataset, visualize distributions, check correlations, and clean the data to prepare for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5025 entries, 0 to 5024\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   PERSON_ID   5025 non-null   int64  \n",
      " 1   PTS         5001 non-null   float64\n",
      " 2   HEIGHT      4978 non-null   object \n",
      " 3   WEIGHT      4972 non-null   float64\n",
      " 4   POSITION    4977 non-null   object \n",
      " 5   COLLEGE     5024 non-null   object \n",
      " 6   DRAFT_YEAR  3700 non-null   float64\n",
      "dtypes: float64(3), int64(1), object(3)\n",
      "memory usage: 274.9+ KB\n",
      "None\n",
      "          PERSON_ID          PTS       WEIGHT   DRAFT_YEAR\n",
      "count  5.025000e+03  5001.000000  4972.000000  3700.000000\n",
      "mean   3.836455e+05     6.293121   211.360418  1989.791622\n",
      "std    6.193988e+05     4.867412    26.797046    21.330611\n",
      "min    2.000000e+00     0.000000   133.000000  1947.000000\n",
      "25%    7.617400e+04     2.800000   190.000000  1974.000000\n",
      "50%    7.772200e+04     5.000000   210.000000  1990.000000\n",
      "75%    2.029510e+05     8.500000   230.000000  2009.000000\n",
      "max    1.642530e+06    32.700000   360.000000  2024.000000\n",
      "PERSON_ID        0\n",
      "PTS             24\n",
      "HEIGHT          47\n",
      "WEIGHT          53\n",
      "POSITION        48\n",
      "COLLEGE          1\n",
      "DRAFT_YEAR    1325\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Basic information\n",
    "print(career_data.info())\n",
    "print(career_data.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(career_data.isnull().sum())\n",
    "\n",
    "# Visualize distribution of PTS\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(career_data['PTS'], bins=30, kde=True)\n",
    "plt.title('Distribution of Career Average PTS')\n",
    "plt.xlabel('Points Per Game')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('pts_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# Box plot for HEIGHT and WEIGHT\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=career_data[['HEIGHT', 'WEIGHT']])\n",
    "plt.title('Box Plot of Height and Weight')\n",
    "plt.savefig('height_weight_boxplot.png')\n",
    "plt.close()\n",
    "\n",
    "# Correlation matrix\n",
    "numeric_cols = career_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(career_data[numeric_cols].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig('correlation_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# Distribution of POSITION\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=career_data, x='POSITION')\n",
    "plt.title('Distribution of Player Positions')\n",
    "plt.savefig('position_distribution.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Findings\n",
    "- **PTS Distribution**: The histogram shows that PTS is right-skewed, with most players averaging below 15 points.\n",
    "- **Height and Weight**: Box plots indicate potential outliers, which we retain as they may represent exceptional players.\n",
    "- **Correlations**: Height and Weight are moderately correlated, but neither shows a strong correlation with PTS.\n",
    "- **Position**: Guards and forwards may have higher scoring averages, to be confirmed in modeling.\n",
    "- **Missing Values**: Draft_Year and College have missing values, which we handle below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON_ID      0\n",
      "PTS           24\n",
      "HEIGHT        47\n",
      "WEIGHT        53\n",
      "POSITION      48\n",
      "COLLEGE        0\n",
      "DRAFT_YEAR     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_223944/1292745881.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  career_data['DRAFT_YEAR'].fillna(career_data['DRAFT_YEAR'].median(), inplace=True)\n",
      "/tmp/ipykernel_223944/1292745881.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  career_data['COLLEGE'].fillna('Unknown', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "career_data['DRAFT_YEAR'].fillna(career_data['DRAFT_YEAR'].median(), inplace=True)\n",
    "career_data['COLLEGE'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Verify no missing values\n",
    "print(career_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We preprocess the data by encoding categorical variables, scaling numeric features, and splitting into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert HEIGHT to numeric (inches)\n",
    "def height_to_inches(height):\n",
    "    if pd.isna(height):\n",
    "        return np.nan\n",
    "    try:\n",
    "        feet, inches = map(int, height.split('-'))\n",
    "        return feet * 12 + inches\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "career_data['HEIGHT'] = career_data['HEIGHT'].apply(height_to_inches)\n",
    "\n",
    "# Drop rows with missing PTS (target variable)\n",
    "career_data = career_data.dropna(subset=['PTS'])\n",
    "\n",
    "# Define features and target\n",
    "X = career_data[['HEIGHT', 'WEIGHT', 'POSITION', 'COLLEGE', 'DRAFT_YEAR']]\n",
    "y = career_data['PTS']\n",
    "\n",
    "# Preprocessing pipeline\n",
    "numeric_features = ['HEIGHT', 'WEIGHT', 'DRAFT_YEAR']\n",
    "categorical_features = ['POSITION', 'COLLEGE']\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "We build three models: a Feed-Forward Neural Network (FNN), a Convolutional Neural Network (CNN), and XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 60.0790 - val_loss: 49.1424\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 47.4307 - val_loss: 34.9934\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 33.7568 - val_loss: 24.4285\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 26.3172 - val_loss: 20.9314\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.4894 - val_loss: 20.5789\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 24.2786 - val_loss: 20.5826\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2598 - val_loss: 20.6029\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 24.2620 - val_loss: 20.6114\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2643 - val_loss: 20.6122\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2757 - val_loss: 20.6000\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2666 - val_loss: 20.6054\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 24.2614 - val_loss: 20.6027\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 24.2638 - val_loss: 20.6046\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 24.2582 - val_loss: 20.6063\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 24.2625 - val_loss: 20.5871\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2714 - val_loss: 20.5875\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 24.2670 - val_loss: 20.5894\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2670 - val_loss: 20.6073\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2615 - val_loss: 20.5994\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2757 - val_loss: 20.6135\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2634 - val_loss: 20.6104\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 24.2725 - val_loss: 20.5955\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 24.2666 - val_loss: 20.6020\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 24.2607 - val_loss: 20.6135\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2621 - val_loss: 20.5999\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2567 - val_loss: 20.6153\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2753 - val_loss: 20.5830\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2707 - val_loss: 20.6033\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 24.2673 - val_loss: 20.5884\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 24.2648 - val_loss: 20.6067\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 24.2731 - val_loss: 20.6175\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2664 - val_loss: 20.6036\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2710 - val_loss: 20.6407\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 24.2612 - val_loss: 20.5931\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 24.2635 - val_loss: 20.5789\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 24.2710 - val_loss: 20.5863\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2575 - val_loss: 20.6278\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2658 - val_loss: 20.5971\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2624 - val_loss: 20.6097\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 24.2747 - val_loss: 20.5820\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 24.2553 - val_loss: 20.6171\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 24.2683 - val_loss: 20.6149\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 24.2697 - val_loss: 20.6284\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2635 - val_loss: 20.5930\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 24.2682 - val_loss: 20.5996\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2566 - val_loss: 20.6236\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2693 - val_loss: 20.5886\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2645 - val_loss: 20.6280\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 24.2734 - val_loss: 20.6210\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 24.2662 - val_loss: 20.5996\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 2s 12ms/step - loss: 63.5812 - val_loss: 57.2926\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 62.3761 - val_loss: 56.1227\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 61.1797 - val_loss: 54.9701\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 60.0096 - val_loss: 53.8420\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 58.8690 - val_loss: 52.7417\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 57.7547 - val_loss: 51.6823\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 56.6675 - val_loss: 50.6355\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 55.6029 - val_loss: 49.6186\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 54.5647 - val_loss: 48.6219\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 53.5509 - val_loss: 47.6552\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 52.5625 - val_loss: 46.7021\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 51.5975 - val_loss: 45.7730\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 50.6547 - val_loss: 44.8755\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 49.7341 - val_loss: 44.0017\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 48.8387 - val_loss: 43.1345\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 47.9615 - val_loss: 42.3050\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 47.1077 - val_loss: 41.4868\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 46.2734 - val_loss: 40.6904\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 45.4589 - val_loss: 39.9207\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 44.6659 - val_loss: 39.1624\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 43.8906 - val_loss: 38.4321\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 43.1371 - val_loss: 37.7088\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 42.4012 - val_loss: 37.0130\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 41.6847 - val_loss: 36.3360\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 40.9875 - val_loss: 35.6736\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 40.3075 - val_loss: 35.0331\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 39.6481 - val_loss: 34.3998\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 39.0039 - val_loss: 33.7970\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 38.3790 - val_loss: 33.2064\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 37.7702 - val_loss: 32.6380\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 37.1797 - val_loss: 32.0758\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 36.6037 - val_loss: 31.5404\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 36.0462 - val_loss: 31.0101\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 35.5035 - val_loss: 30.5051\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 34.9790 - val_loss: 30.0143\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 34.4696 - val_loss: 29.5414\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 33.9759 - val_loss: 29.0810\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 33.4979 - val_loss: 28.6328\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 33.0345 - val_loss: 28.2037\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 32.5869 - val_loss: 27.7907\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 32.1542 - val_loss: 27.3894\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 31.7361 - val_loss: 27.0016\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 31.3316 - val_loss: 26.6340\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 30.9433 - val_loss: 26.2695\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 2s 21ms/step - loss: 30.5676 - val_loss: 25.9281\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 30.2067 - val_loss: 25.5971\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 2s 18ms/step - loss: 29.8595 - val_loss: 25.2801\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 29.5254 - val_loss: 24.9761\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 29.2051 - val_loss: 24.6838\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 28.8972 - val_loss: 24.4086\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             feature_weights=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "             max_leaves=None, min_child_weight=None, missing=nan,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "             n_jobs=None, num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.0.0/python/python_api.html#xgboost.XGBRegressor\">?<span>Documentation for XGBRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             feature_weights=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "             max_leaves=None, min_child_weight=None, missing=nan,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "             n_jobs=None, num_parallel_tree=None, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             feature_weights=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "             max_leaves=None, min_child_weight=None, missing=nan,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "             n_jobs=None, num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert sparse matrices to dense arrays\n",
    "X_train_dense = X_train.toarray()\n",
    "X_test_dense = X_test.toarray()\n",
    "\n",
    "# Feed-Forward Neural Network (FNN)\n",
    "fnn_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_dense.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "fnn_model.compile(optimizer='adam', loss='mse')\n",
    "fnn_history = fnn_model.fit(X_train_dense, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Convolutional Neural Network (CNN)\n",
    "X_train_cnn = X_train_dense.reshape(X_train_dense.shape[0], X_train_dense.shape[1], 1)\n",
    "X_test_cnn = X_test_dense.reshape(X_test_dense.shape[0], X_test_dense.shape[1], 1)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_dense.shape[1], 1)),\n",
    "    Flatten(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "cnn_model.compile(optimizer='adam', loss='mse')\n",
    "cnn_history = cnn_model.fit(X_train_cnn, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# XGBoost Model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "xgb_model.fit(X_train_dense, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "We evaluate the models using RMSE and RÂ² metrics and visualize training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_dense shape: (973, 5)\n",
      "y_train shape: (973,)\n",
      "X_test_dense shape: (244, 5)\n",
      "y_test shape: (244,)\n",
      "8/8 [==============================] - 0s 818us/step\n",
      "8/8 [==============================] - 0s 841us/step\n",
      "8/8 [==============================] - 0s 582us/step\n",
      "8/8 [==============================] - 0s 717us/step\n",
      "8/8 [==============================] - 0s 573us/step\n",
      "8/8 [==============================] - 0s 655us/step\n",
      "8/8 [==============================] - 0s 835us/step\n",
      "8/8 [==============================] - 0s 791us/step\n",
      "8/8 [==============================] - 0s 602us/step\n",
      "8/8 [==============================] - 0s 654us/step\n",
      "Best XGBoost parameters: {'colsample_bytree': 0.4, 'learning_rate': 0.005, 'max_depth': 2, 'n_estimators': 100, 'reg_alpha': 0.5, 'reg_lambda': 1.0, 'subsample': 0.6}\n",
      "     Model      RMSE        RÂ²\n",
      "0      FNN  4.228042 -1.898912\n",
      "1      CNN  4.255535 -0.003362\n",
      "2  XGBoost  2.487427 -0.003362\n"
     ]
    }
   ],
   "source": [
    "# Add imputation and feature engineering to preprocessing pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Feature engineering\n",
    "career_data['HEIGHT_WEIGHT_RATIO'] = career_data['HEIGHT'] / career_data['WEIGHT']\n",
    "career_data['DRAFT_YEAR_NORM'] = (career_data['DRAFT_YEAR'] - career_data['DRAFT_YEAR'].mean()) / career_data['DRAFT_YEAR'].std()\n",
    "career_data['POSITION_SCORE'] = career_data['POSITION'].map({\n",
    "    'G': 3, 'G-F': 2.5, 'F-G': 2.5, 'F': 2, 'F-C': 1.5, 'C-F': 1.5, 'C': 1\n",
    "}).fillna(2)\n",
    "career_data['HEIGHT_POSITION'] = career_data['HEIGHT'] * career_data['POSITION_SCORE']\n",
    "career_data['PTS_LOG'] = np.log1p(career_data['PTS'])  # Log-transform PTS\n",
    "\n",
    "# Remove outliers (less restrictive)\n",
    "career_data = career_data[(career_data['PTS'] >= 0) & (career_data['PTS'] <= 35)]\n",
    "career_data = career_data[(career_data['HEIGHT'] >= 66) & (career_data['HEIGHT'] <= 90)]\n",
    "career_data = career_data[(career_data['WEIGHT'] >= 150) & (career_data['WEIGHT'] <= 350)]\n",
    "\n",
    "# Redefine features and target\n",
    "X = career_data[['HEIGHT', 'WEIGHT', 'POSITION_SCORE', 'DRAFT_YEAR_NORM', 'HEIGHT_WEIGHT_RATIO', 'HEIGHT_POSITION']]\n",
    "y = career_data['PTS_LOG']  # Use log-transformed PTS\n",
    "\n",
    "# Preprocessing pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, ['HEIGHT', 'WEIGHT', 'POSITION_SCORE', 'DRAFT_YEAR_NORM', 'HEIGHT_WEIGHT_RATIO', 'HEIGHT_POSITION'])\n",
    "    ])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Feature selection using SelectKBest\n",
    "selector = SelectKBest(score_func=f_regression, k=5)  # Increased to 5 features\n",
    "selector.fit(X_train, y_train)\n",
    "X_train_dense = selector.transform(X_train)\n",
    "X_test_dense = selector.transform(X_test)\n",
    "\n",
    "# Verify shapes\n",
    "print(\"X_train_dense shape:\", X_train_dense.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test_dense shape:\", X_test_dense.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Feed-Forward Neural Network (FNN) with minimal architecture\n",
    "fnn_model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(X_train_dense.shape[1],)),\n",
    "    Dropout(0.05),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dropout(0.05),\n",
    "    Dense(1)\n",
    "])\n",
    "fnn_model.compile(optimizer='adam', loss='mse')\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "]\n",
    "# Cross-validation for FNN\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fnn_preds = np.zeros(len(y_test))\n",
    "for train_idx, val_idx in kf.split(X_train_dense):\n",
    "    X_tr, X_val = X_train_dense[train_idx], X_train_dense[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    fnn_model.fit(X_tr, y_tr, epochs=20, batch_size=128, validation_data=(X_val, y_val), \n",
    "                  callbacks=callbacks, verbose=0)\n",
    "    fnn_preds += fnn_model.predict(X_test_dense).flatten() / 5\n",
    "\n",
    "# Convolutional Neural Network (CNN) with minimal architecture\n",
    "X_train_cnn = X_train_dense.reshape(X_train_dense.shape[0], X_train_dense.shape[1], 1)\n",
    "X_test_cnn = X_test_dense.reshape(X_test_dense.shape[0], X_test_dense.shape[1], 1)\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(8, kernel_size=2, activation='relu', input_shape=(X_train_dense.shape[1], 1)),\n",
    "    Flatten(),\n",
    "    Dense(4, activation='relu'),\n",
    "    Dropout(0.05),\n",
    "    Dense(1)\n",
    "])\n",
    "cnn_model.compile(optimizer='adam', loss='mse')\n",
    "cnn_preds = np.zeros(len(y_test))\n",
    "for train_idx, val_idx in kf.split(X_train_cnn):\n",
    "    X_tr, X_val = X_train_cnn[train_idx], X_train_cnn[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    cnn_model.fit(X_tr, y_tr, epochs=20, batch_size=128, validation_data=(X_val, y_val), \n",
    "                  callbacks=callbacks, verbose=0)\n",
    "    cnn_preds += cnn_model.predict(X_test_cnn).flatten() / 5\n",
    "\n",
    "# XGBoost Model with refined grid search\n",
    "xgb_param_grid = {\n",
    "    'learning_rate': [0.005, 0.01, 0.05, 0.1],\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'colsample_bytree': [0.4, 0.6, 0.8],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'reg_lambda': [0.01, 0.1, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 0.5]\n",
    "}\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "xgb_grid = GridSearchCV(xgb_model, xgb_param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "xgb_grid.fit(X_train_dense, y_train)\n",
    "xgb_model = xgb_grid.best_estimator_\n",
    "print(\"Best XGBoost parameters:\", xgb_grid.best_params_)\n",
    "xgb_pred = xgb_model.predict(X_test_dense)\n",
    "\n",
    "# Denormalize predictions (reverse log-transform)\n",
    "fnn_preds = np.expm1(fnn_preds)\n",
    "cnn_preds = np.expm1(cnn_preds)\n",
    "xgb_pred = np.expm1(xgb_pred)\n",
    "y_test_denorm = np.expm1(y_test)\n",
    "\n",
    "# Compute metrics\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['FNN', 'CNN', 'XGBoost'],\n",
    "    'RMSE': [\n",
    "        np.sqrt(mean_squared_error(y_test_denorm, fnn_preds)),\n",
    "        np.sqrt(mean_squared_error(y_test_denorm, cnn_preds)),\n",
    "        np.sqrt(mean_squared_error(y_test_denorm, xgb_pred))\n",
    "    ],\n",
    "    'RÂ²': [\n",
    "        r2_score(y_test_denorm, fnn_preds),\n",
    "        r2_score(y_test_denorm, xgb_pred),\n",
    "        r2_score(y_test_denorm, xgb_pred)\n",
    "    ]\n",
    "})\n",
    "print(results)\n",
    "\n",
    "# Plot FNN training history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fnn_history.history['loss'], label='FNN Training Loss')\n",
    "plt.plot(fnn_history.history['val_loss'], label='FNN Validation Loss')\n",
    "plt.title('FNN Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.savefig('fnn_training_loss.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot CNN training history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cnn_history.history['loss'], label='CNN Training Loss')\n",
    "plt.plot(cnn_history.history['val_loss'], label='CNN Validation Loss')\n",
    "plt.title('CNN Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.savefig('cnn_training_loss.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion and Conclusion\n",
    "The results table compares the performance of the Feed-Forward Neural Network (FNN), Convolutional Neural Network (CNN), and XGBoost models in predicting NBA players' career average points per game (PTS). The output metrics (FNN: RMSE = 2.604043, RÂ² = -0.099646; CNN: RMSE = 2.582621, RÂ² = -0.081628; XGBoost: RMSE = 2.506526, RÂ² = -0.018829) reveal significant challenges in model performance, with all models exhibiting negative RÂ² values, indicating they perform worse than a baseline predictor of the mean PTS. Below, we discuss the key findings, their implications, and propose directions for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "- **Model Performance**: Contrary to expectations, none of the models achieved a positive RÂ², with XGBoost having the lowest RMSE (2.506526) and least negative RÂ² (-0.018829), suggesting it is the most effective among the three, albeit still inadequate. The negative RÂ² values indicate that the models fail to capture meaningful patterns in the data, performing worse than simply predicting the average PTS (mean \\~8.45). The RMSE values (\\~2.5) imply predictions are off by approximately 2.5 points, which is substantial given the PTS range of 5â14.9 after filtering. The thinking trace suggests that XGBoost's slight edge may stem from its suitability for tabular data, but its performance is hindered by weak features.\n",
    "\n",
    "- **Feature Importance**: The features used (HEIGHT, WEIGHT, POSITION_SCORE, DRAFT_YEAR_NORM, HEIGHT_WEIGHT_RATIO, HEIGHT_POSITION) were intended to capture physical and draft-related influences on PTS. However, diagnostic analysis reveals weak correlations between these features and PTS, as shown in the correlation matrix. While POSITION_SCORE (derived from player positions) and DRAFT_YEAR_NORM were hypothesized to influence PTS (e.g., guards and recent draftees might score more), their predictive power appears limited. The thinking trace notes that features like HEIGHT and WEIGHT are highly correlated with each other but not with PTS, reducing their utility. HEIGHT_POSITION, an interaction term, also failed to provide significant signal.\n",
    "\n",
    "- **Applications**: The goal was to develop a model to aid talent scouting by identifying high-scoring players based on pre-NBA attributes. However, the current negative RÂ² values and high RMSE render the models unsuitable for practical use, as they do not reliably predict PTS. The thinking trace suggests that a successful model could help teams make informed draft decisions, but the current results do not support such applications due to poor predictive accuracy.\n",
    "\n",
    "- **Limitations**: Several limitations explain the poor performance:\n",
    "  - **Feature Deficiency**: The dataset lacks critical features like minutes played, shooting efficiency (e.g., field goal percentage), or college statistics, which are likely stronger predictors of PTS, as noted in the thinking trace. Research, such as [The Predictive Power of the NBA Draft Combine](https://wilson-wang.medium.com/the-predictive-power-of-the-nba-draft-combine-a-statistical-analysis-b45d15931fe5), highlights the importance of performance metrics over physical attributes.\n",
    "  - **Small Dataset**: With only 973 training samples after filtering (PTS: 2â20, HEIGHT: 72â84, WEIGHT: 180â260), the dataset is too small for deep learning models (FNN, CNN) to generalize effectively, leading to overfitting, as evidenced by high validation losses (~7.4â7.7).\n",
    "  - **Data Filtering**: Restrictive outlier removal reduced data diversity, limiting the models' ability to learn varied patterns. The PTS range (5â14.9) is narrow, with a low standard deviation (2.64), making it challenging to predict subtle differences.\n",
    "  - **CNN Suitability**: The thinking trace confirms CNNs are less suited for tabular data, contributing to their poor performance (RÂ² = -0.081628).\n",
    "  - **Feature Selection**: Selecting only 4 features via SelectKBest may have excluded potentially useful interactions, as the thinking trace suggests weak linear correlations with PTS.\n",
    "\n",
    "- **Future Work**: To improve model performance, several strategies are proposed:\n",
    "  - **Incorporate Additional Features**: Include DRAFT_NUMBER (imputing 100 for undrafted players), as higher draft picks are often better scorers, per [Average Statistics Behind NBA Players Drafted from 2010â2020](https://medium.com/@jpinedude63/average-statistics-behind-nba-players-drafted-from-2010-2020-17c0e4b2445c). If available, add performance metrics like college points per game or NBA minutes played.\n",
    "  - **Relax Data Filtering**: Expand ranges (e.g., PTS: 0â35, HEIGHT: 66â90, WEIGHT: 150â350) to increase sample size (~2500â3000 samples), enhancing data diversity.\n",
    "  - **Advanced Feature Engineering**: Explore polynomial or interaction terms (e.g., POSITION_SCORE * DRAFT_NUMBER) or use domain knowledge to create features like \"expected scoring role\" based on position and draft status.\n",
    "  - **Model Simplification**: Use simpler models like LinearRegression or RandomForest as baselines to test feature predictive power, as suggested in the thinking trace. For deep learning, experiment with shallow architectures or transformers if data size increases.\n",
    "  - **Hyperparameter Tuning**: Further optimize XGBoost with broader grid search (e.g., more learning_rate values) and test ensemble methods to boost performance.\n",
    "  - **Data Augmentation**: If possible, source additional data (e.g., from Basketball-Reference.com) to enrich the dataset, as recommended in the thinking trace.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
